{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7f352145",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloaded modules: ['recsys', 'recsys.recommendation.MFRecommender', 'recsys.recommendation.recommend', 'recsys.recommendation']\n"
          ]
        }
      ],
      "source": [
        "# Use this cell when you modify the recsys package and need to reload it\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Reload the recsys modules\n",
        "modules_to_reload = []\n",
        "for module_name in sys.modules.keys():\n",
        "    if module_name.startswith('recsys'):\n",
        "        modules_to_reload.append(module_name)\n",
        "\n",
        "if modules_to_reload:\n",
        "    # Reload in reverse order to handle dependencies\n",
        "    for module_name in reversed(sorted(modules_to_reload)):\n",
        "        importlib.reload(sys.modules[module_name])\n",
        "    print(f\"Reloaded modules: {modules_to_reload}\")\n",
        "else:\n",
        "    print(\"No recsys modules found to reload\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984e94b5",
      "metadata": {},
      "source": [
        "## Matrix Factorization Recommendation Engine\n",
        "This notebook will document the implementation of the Matrix Factorization Method to solving item recommendation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0535e88",
      "metadata": {},
      "source": [
        "## Dataset that we are using\n",
        "The dataset that we will be using for this will be obtained from MovieLens (https://grouplens.org/datasets/movielens/).\n",
        "\n",
        "This dataset holds approximately 33,000,000 ratings and 2,000,000 tag applications applied to 86,000 movies by 330,975 users."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709aa4c4",
      "metadata": {},
      "source": [
        "## Initial downloading of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4b07580d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9f92f361",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "project_root = Path(\"..\").resolve()\n",
        "if str(project_root / \"src\") not in sys.path:\n",
        "    sys.path.append(str(project_root / \"src\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7f183291",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/Users/tonyli/Documents/Projects/craftyverse/craftyverse-recsys/data/processed')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from recsys.utils import unzip_file\n",
        "zip_path = project_root / \"data\" / \"raw\" / \"ml-latest.zip\"\n",
        "destination = project_root / \"data\" / \"processed\"\n",
        "\n",
        "unzip_file(zip_path, destination, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6425ac9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   userId  movieId  rating   timestamp\n",
            "0       1        1     4.0  1225734739\n",
            "1       1      110     4.0  1225865086\n",
            "2       1      158     4.0  1225733503\n",
            "3       1      260     4.5  1225735204\n",
            "4       1      356     5.0  1225735119\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from recsys.utils import read_csv\n",
        "csv_path = project_root / \"data\" /\"processed\" / \"ml-latest\" / \"ratings.csv\"\n",
        "\n",
        "ratings_df = read_csv(csv_path)\n",
        "print(ratings_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db881bdd",
      "metadata": {},
      "source": [
        "## Encoding the data\n",
        "The first step we want to have contiguous ids for users and movies. This means that we want to have a unique identifier that can identify both user and movie. In this dataset we have the `timestamp` as the contiguous id.\n",
        "\n",
        "Then we'll then split the ratings dataset into two subsets:\n",
        "  - Training Dataset (75% of the data)\n",
        "  - Testing Dataset (15% of the data)\n",
        "  - Validation Dataset (10% of the data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "afd68372",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training threshold (75th percentile): 1496919379.25\n",
            "Testing threshold (90th percentile): 1598691985.6000001\n",
            "Validation will be the remaining 10% (90th-100th percentile)\n",
            "      userId  movieId  rating   timestamp\n",
            "1538      22       16     3.5  1685231200\n",
            "1545      22      165     3.5  1685231212\n",
            "1547      22      260     4.0  1685231007\n",
            "1548      22      288     4.5  1685230899\n",
            "1554      22     1036     4.0  1685230896\n",
            "\n",
            "Dataset split statistics:\n",
            "Total ratings: 33,832,162\n",
            "Training set: 25,374,121 (75.0%)\n",
            "Testing set: 5,074,824 (15.0%)\n",
            "Validation set: 3,383,217 (10.0%)\n",
            "      userId  movieId  rating   timestamp\n",
            "1538      22       16     3.5  1685231200\n",
            "1545      22      165     3.5  1685231212\n",
            "1547      22      260     4.0  1685231007\n",
            "1548      22      288     4.5  1685230899\n",
            "1554      22     1036     4.0  1685230896\n",
            "\n",
            "Dataset split statistics:\n",
            "Total ratings: 33,832,162\n",
            "Training set: 25,374,121 (75.0%)\n",
            "Testing set: 5,074,824 (15.0%)\n",
            "Validation set: 3,383,217 (10.0%)\n"
          ]
        }
      ],
      "source": [
        "# Calculate quantiles for splitting the dataset\n",
        "# 75% for training, 15% for testing, 10% for validation\n",
        "train_threshold = np.quantile(ratings_df.timestamp, 0.75)  # 75th percentile\n",
        "test_threshold = np.quantile(ratings_df.timestamp, 0.90)   # 90th percentile (75% + 15%)\n",
        "\n",
        "print(f\"Training threshold (75th percentile): {train_threshold}\")\n",
        "print(f\"Testing threshold (90th percentile): {test_threshold}\")\n",
        "print(f\"Validation will be the remaining 10% (90th-100th percentile)\")\n",
        "\n",
        "# Split the dataset based on timestamps\n",
        "train_df = ratings_df[ratings_df.timestamp <= train_threshold].copy()\n",
        "test_df = ratings_df[(ratings_df.timestamp > train_threshold) & \n",
        "                     (ratings_df.timestamp <= test_threshold)].copy()\n",
        "val_df = ratings_df[ratings_df.timestamp > test_threshold].copy()\n",
        "\n",
        "print(val_df.head())\n",
        "\n",
        "# Print split statistics\n",
        "total_ratings = len(ratings_df)\n",
        "print(f\"\\nDataset split statistics:\")\n",
        "print(f\"Total ratings: {total_ratings:,}\")\n",
        "print(f\"Training set: {len(train_df):,} ({len(train_df)/total_ratings*100:.1f}%)\")\n",
        "print(f\"Testing set: {len(test_df):,} ({len(test_df)/total_ratings*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(val_df):,} ({len(val_df)/total_ratings*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3351d1",
      "metadata": {},
      "source": [
        "## Dataset Cleaning\n",
        "This section will clean the partitioned datasets that we conducted in the previous step to prepare them for matrix factorization.\n",
        "\n",
        "### Cleaning Steps Overview:\n",
        "\n",
        "1. **Create Contiguous ID Mappings**\n",
        "   - Map original user IDs → contiguous indices (0, 1, 2, ...)\n",
        "   - Map original movie IDs → contiguous indices (0, 1, 2, ...)\n",
        "   - This ensures memory efficiency and algorithm compatibility\n",
        "\n",
        "2. **Handle Cold Start Problems**\n",
        "   - Filter out users/movies that appear only in test/validation sets\n",
        "   - Ensure all test/validation users and movies exist in training data\n",
        "   - Remove ratings for unknown users/movies from test/validation sets\n",
        "\n",
        "3. **Create Rating Matrices**\n",
        "   - Build sparse user-item rating matrix for training\n",
        "   - Convert test/validation data to use the same index mappings\n",
        "   - Handle missing ratings (typically filled with 0 or mean ratings)\n",
        "\n",
        "4. **Data Quality Checks**\n",
        "   - Verify no data leakage between train/test/validation\n",
        "   - Check matrix dimensions and sparsity\n",
        "   - Validate that all indices are within expected ranges"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9568c79",
      "metadata": {},
      "source": [
        "First we want to compute the unique userIds in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5bf61222",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1  2  4  5  6  7  8  9 10 11 12 14 15 16 17]\n",
            "Number of unique users in training set: 268300\n"
          ]
        }
      ],
      "source": [
        "# Compute number of unique users in the training dataset\n",
        "training_user_ids = np.sort(np.unique(train_df.userId.values))\n",
        "n_training_users = len(training_user_ids)\n",
        "print(training_user_ids[:15])\n",
        "print(f\"Number of unique users in training set: {n_training_users}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc13d2d",
      "metadata": {},
      "source": [
        "### Why do we need userId_to_index mapping?\n",
        "\n",
        "In matrix factorization, we need to create matrices where:\n",
        "- **Rows represent users**\n",
        "- **Columns represent movies** \n",
        "- **Values are ratings**\n",
        "\n",
        "**The Problem**: User IDs in MovieLens are not contiguous integers starting from 0. For example, you might have user IDs like [1, 5, 7, 15, 23, ...] instead of [0, 1, 2, 3, 4, ...].\n",
        "\n",
        "**Why this matters for Matrix Factorization**:\n",
        "1. **Memory efficiency**: If your largest user ID is 330,975 but you only have 100,000 users, creating a matrix with 330,975 rows would waste ~70% of memory on empty rows\n",
        "2. **Algorithm requirements**: Most matrix factorization libraries (like scikit-learn's NMF, or custom implementations) expect matrix indices to be contiguous starting from 0\n",
        "3. **Performance**: Sparse matrices and mathematical operations are more efficient with contiguous indices\n",
        "\n",
        "**The Solution**: Create a mapping from original user IDs to contiguous indices (0, 1, 2, ..., n_users-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8c4ec525",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Original user ID range: 1 to 330975\n",
            "Matrix index range:     0 to 268299\n",
            "Memory saved: 62,676 empty rows avoided\n"
          ]
        }
      ],
      "source": [
        "# Create mapping from original user IDs to contiguous matrix indices\n",
        "userId_to_index = {user_id: index for index, user_id in enumerate(training_user_ids)}\n",
        "\n",
        "print(f\"\\nOriginal user ID range: {training_user_ids.min()} to {training_user_ids.max()}\")\n",
        "print(f\"Matrix index range:     0 to {len(training_user_ids)-1}\")\n",
        "print(f\"Memory saved: {((training_user_ids.max() + 1) - len(training_user_ids)):,} empty rows avoided\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fcd5db03",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Memory efficiency:\n",
            "Original matrix size: 57,548,782,976 elements\n",
            "Optimized matrix size: 11,064,423,700 elements\n",
            "Memory saved: 80.8%\n"
          ]
        }
      ],
      "source": [
        "# Similarly, we need to create a mapping for movie IDs\n",
        "training_movie_ids = np.sort(np.unique(train_df.movieId.values))\n",
        "movieId_to_index = {movie_id: index for index, movie_id in enumerate(training_movie_ids)}\n",
        "\n",
        "# Calculate memory savings\n",
        "original_size = (training_user_ids.max() + 1) * (training_movie_ids.max() + 1)\n",
        "optimized_size = len(training_user_ids) * len(training_movie_ids)\n",
        "savings_percentage = (1 - optimized_size/original_size) * 100\n",
        "\n",
        "print(f\"\\nMemory efficiency:\")\n",
        "print(f\"Original matrix size: {original_size:,} elements\")\n",
        "print(f\"Optimized matrix size: {optimized_size:,} elements\") \n",
        "print(f\"Memory saved: {savings_percentage:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c316dfa1",
      "metadata": {},
      "source": [
        "### Step 1: Apply ID Mappings to Training Data\n",
        "\n",
        "Now that we have our mappings created, we need to **transform the actual data** to use the new contiguous indices instead of the original IDs.\n",
        "\n",
        "**What this code does:**\n",
        "- Replaces each original `userId` in the training dataframe with its corresponding matrix index\n",
        "- For example: original userId `123` becomes matrix index `5` (if it's the 6th unique user)\n",
        "- This creates a \"clean\" dataset where all user IDs are contiguous integers starting from 0\n",
        "\n",
        "**Why this step is crucial:**\n",
        "- The training data now has user IDs that can directly index into our rating matrix\n",
        "- No more gaps or missing indices - every user ID from 0 to n_users-1 exists\n",
        "- This transformed data is what we'll use to build our user-item rating matrix for matrix factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480e9dd7",
      "metadata": {},
      "source": [
        "#### For UserId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a9021fb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   userId  movieId  rating   timestamp\n",
            "0       0        1     4.0  1225734739\n",
            "1       0      110     4.0  1225865086\n",
            "2       0      158     4.0  1225733503\n",
            "3       0      260     4.5  1225735204\n",
            "4       0      356     5.0  1225735119\n"
          ]
        }
      ],
      "source": [
        "train_df['userId'] = train_df['userId'].apply(lambda uid: userId_to_index[uid])\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "116058e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed test set (with unknown users removed):\n",
            "      userId  movieId  rating   timestamp\n",
            "8138      51     3822     3.5  1514820246\n",
            "8142      51     4979     4.0  1514916004\n",
            "8143      51     6043     4.0  1514820568\n",
            "8144      51     6711     3.5  1514821550\n",
            "8147      51     7068     5.0  1527964516\n"
          ]
        }
      ],
      "source": [
        "# Now we need to transform the test set\n",
        "test_df['userId'] = test_df['userId'].apply(lambda uid: userId_to_index.get(uid, -1))\n",
        "test_df = test_df[test_df['userId'] >= 0].copy()  # Remove unknown users\n",
        "print(\"Transformed test set (with unknown users removed):\")\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cef43b38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed validation set (with unknown users deleted):\n",
            "       userId  movieId  rating   timestamp\n",
            "11832     109     1394     4.0  1668055512\n",
            "11879     109     3362     4.0  1609548629\n",
            "11901     109     4370     4.5  1598921146\n",
            "11924     109     5772     5.0  1615761089\n",
            "11938     109     6787     4.0  1599104418\n"
          ]
        }
      ],
      "source": [
        "# Transform the validation set\n",
        "val_df['userId'] = val_df['userId'].apply(lambda uid: userId_to_index.get(uid, -1))\n",
        "val_df = val_df[val_df['userId'] >= 0].copy()  # Remove unknown users\n",
        "print(\"Transformed validation set (with unknown users deleted):\")\n",
        "print(val_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b5b18a1",
      "metadata": {},
      "source": [
        "#### For MovieID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c52aadb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   userId  movieId  rating   timestamp\n",
            "0       0        0     4.0  1225734739\n",
            "1       0      108     4.0  1225865086\n",
            "2       0      156     4.0  1225733503\n",
            "3       0      257     4.5  1225735204\n",
            "4       0      351     5.0  1225735119\n"
          ]
        }
      ],
      "source": [
        "train_df['movieId'] = train_df['movieId'].apply(lambda mid: movieId_to_index[mid])\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "adb9d40b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed test set (with unknown movies deleted):\n",
            "      userId  movieId  rating   timestamp\n",
            "8138      51     3721     3.5  1514820246\n",
            "8142      51     4874     4.0  1514916004\n",
            "8143      51     5932     4.0  1514820568\n",
            "8144      51     6589     3.5  1514821550\n",
            "8147      51     6944     5.0  1527964516\n"
          ]
        }
      ],
      "source": [
        "test_df['movieId'] = test_df['movieId'].apply(lambda mid: movieId_to_index.get(mid, -1))\n",
        "test_df = test_df[test_df['movieId'] >= 0].copy()  # Remove unknown movies\n",
        "print(\"Transformed test set (with unknown movies deleted):\")\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4204e3f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed validation set (with unknown movies deleted):\n",
            "       userId  movieId  rating   timestamp\n",
            "11832     109     1358     4.0  1668055512\n",
            "11879     109     3269     4.0  1609548629\n",
            "11901     109     4266     4.5  1598921146\n",
            "11924     109     5661     5.0  1615761089\n",
            "11938     109     6665     4.0  1599104418\n"
          ]
        }
      ],
      "source": [
        "val_df['movieId'] = val_df['movieId'].apply(lambda mid: movieId_to_index.get(mid, -1))\n",
        "val_df = val_df[val_df['movieId'] >= 0].copy()  # Remove unknown movies\n",
        "print(\"Transformed validation set (with unknown movies deleted):\")\n",
        "print(val_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec0bed9",
      "metadata": {},
      "source": [
        "## Embedding the Data\n",
        "\n",
        "Now that we have clean, indexed data, we need to create the **user-item rating matrix** that will be decomposed through matrix factorization.\n",
        "\n",
        "### What are Embeddings in Matrix Factorization?\n",
        "\n",
        "**Matrix Factorization** decomposes our large, sparse user-item rating matrix into two smaller dense matrices:\n",
        "- **user_embeddings** (User latent factors): Each row represents a user as a vector of preferences\n",
        "- **movie_embeddings** (Movie latent factors): Each row represents a movie as a vector of characteristics\n",
        "\n",
        "**Mathematical Representation:**\n",
        "```\n",
        "rating_matrix ≈ user_embeddings × movie_embeddings^T\n",
        "```\n",
        "Where:\n",
        "- **rating_matrix** is our `num_users × num_movies` rating matrix (mostly zeros/missing values)\n",
        "- **user_embeddings** is `num_users × num_latent_factors` matrix (typically 50-200 factors)\n",
        "- **movie_embeddings** is `num_movies × num_latent_factors` matrix\n",
        "- **num_latent_factors** is much smaller than both num_users and num_movies\n",
        "\n",
        "### Why Embeddings Work\n",
        "\n",
        "The key insight is that user preferences and movie characteristics can be represented in a **lower-dimensional latent space**. For example:\n",
        "- **Latent Factor 1**: \"Action vs Romance preference\"\n",
        "- **Latent Factor 2**: \"Mainstream vs Indie preference\"  \n",
        "- **Latent Factor 3**: \"New vs Classic movie preference\"\n",
        "\n",
        "Each user and movie gets a vector in this latent space, and their **dot product** predicts the rating.\n",
        "\n",
        "### Next Steps\n",
        "1. Create the sparse user-item rating matrix from our cleaned data\n",
        "2. Initialize user_embeddings and movie_embeddings matrices\n",
        "3. Train the model to minimize prediction error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "75f01ff0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253ba269",
      "metadata": {},
      "source": [
        "### Understanding PyTorch Embeddings with a Simple Example\n",
        "\n",
        "Before we build our full matrix factorization model, let's understand how **PyTorch embeddings** work with a simple example.\n",
        "\n",
        "**What `nn.Embedding` does:**\n",
        "- Creates a lookup table that maps integer indices to dense vectors\n",
        "- Each row in the embedding matrix represents one entity (user or movie)\n",
        "- The vectors are **learnable parameters** that get optimized during training\n",
        "\n",
        "**In our example below:**\n",
        "- `n_users = 5` means we have 5 users (with indices 0, 1, 2, 3, 4)\n",
        "- `embed_size = 5` means each user is represented by a 5-dimensional vector\n",
        "- `embed.weight` is a 5×5 matrix where each row is a user's embedding vector\n",
        "\n",
        "**How it works:**\n",
        "```python\n",
        "# If we want the embedding for user 0:\n",
        "user_0_embedding = embed(torch.tensor([0]))  # Returns the first row\n",
        "user_3_embedding = embed(torch.tensor([3]))  # Returns the fourth row\n",
        "```\n",
        "\n",
        "**Why this is powerful:**\n",
        "- Initially, the embeddings are **random numbers** (as you'll see below)\n",
        "- During training, these numbers get adjusted to capture user preferences\n",
        "- Similar users will end up with similar embedding vectors\n",
        "- The model learns meaningful representations automatically!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "833f88dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial random embedding matrix:\n",
            "Shape: torch.Size([5, 5])\n",
            "Values:\n",
            "tensor([[ 0.7646,  1.0155,  0.6477,  1.4862, -0.9355],\n",
            "        [-0.1059, -1.0354, -0.0275,  0.9706,  0.5976],\n",
            "        [ 0.4763,  0.1567,  0.9565,  1.0822, -0.1728],\n",
            "        [-0.9543, -0.5378, -0.7715,  0.5377, -0.6616],\n",
            "        [-0.8066,  1.0206, -0.6346, -0.5660, -0.6606]])\n",
            "\n",
            "==================================================\n",
            "Getting embeddings for individual users:\n",
            "User 0 embedding: tensor([[ 0.7646,  1.0155,  0.6477,  1.4862, -0.9355]])\n",
            "User 3 embedding: tensor([[-0.9543, -0.5378, -0.7715,  0.5377, -0.6616]])\n",
            "\n",
            "Embeddings for users [0, 2, 4]:\n",
            "tensor([[ 0.7646,  1.0155,  0.6477,  1.4862, -0.9355],\n",
            "        [ 0.4763,  0.1567,  0.9565,  1.0822, -0.1728],\n",
            "        [-0.8066,  1.0206, -0.6346, -0.5660, -0.6606]])\n"
          ]
        }
      ],
      "source": [
        "# Create a simple embedding example\n",
        "embed_size = 5  # Each user/movie will be represented by 5 numbers\n",
        "n_users = 5     # We have 5 users (indices 0, 1, 2, 3, 4)\n",
        "\n",
        "# Create the embedding layer\n",
        "user_embeddings = nn.Embedding(n_users, embed_size)\n",
        "\n",
        "print(\"Initial random embedding matrix:\")\n",
        "print(\"Shape:\", user_embeddings.weight.shape)\n",
        "print(\"Values:\")\n",
        "print(user_embeddings.weight.data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Example: Get embeddings for specific users\n",
        "print(\"Getting embeddings for individual users:\")\n",
        "\n",
        "# Get embedding for user 0\n",
        "user_0_vector = user_embeddings(torch.tensor([0]))\n",
        "print(f\"User 0 embedding: {user_0_vector.data}\")\n",
        "\n",
        "# Get embedding for user 3  \n",
        "user_3_vector = user_embeddings(torch.tensor([3]))\n",
        "print(f\"User 3 embedding: {user_3_vector.data}\")\n",
        "\n",
        "# Get embeddings for multiple users at once\n",
        "multiple_users = user_embeddings(torch.tensor([0, 2, 4]))\n",
        "print(f\"\\nEmbeddings for users [0, 2, 4]:\")\n",
        "print(multiple_users.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc23188f",
      "metadata": {},
      "source": [
        "### Building the Complete Matrix Factorization Model\n",
        "\n",
        "Now let's build our **complete recommendation system** using PyTorch. The `MFRecommender` class implements the core matrix factorization algorithm.\n",
        "\n",
        "**Class Architecture:**\n",
        "```\n",
        "MFRecommender\n",
        "├── user_embedding: Maps user IDs → user preference vectors\n",
        "├── item_embedding: Maps movie IDs → movie characteristic vectors  \n",
        "└── forward(): Computes predicted ratings via dot products\n",
        "```\n",
        "\n",
        "**How it works step-by-step:**\n",
        "\n",
        "1. **Initialization (`__init__`)**:\n",
        "   - Creates two embedding lookup tables (user and movie)\n",
        "   - Each embedding has `embed_size` dimensions (typically 50-200)\n",
        "   - Embeddings start with random weights that get learned during training\n",
        "\n",
        "2. **Forward Pass (`forward`)**:\n",
        "   - Takes user IDs and movie IDs as input\n",
        "   - Looks up their corresponding embedding vectors\n",
        "   - Computes **dot product** between user and movie vectors\n",
        "   - Returns predicted ratings\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "predicted_rating = user_vector · movie_vector\n",
        "                 = Σ(user_embedding[i] × movie_embedding[i])\n",
        "```\n",
        "\n",
        "**Key Insight**: The dot product measures **similarity** between user preferences and movie characteristics in the latent space. High dot product = high predicted rating!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4df3a659",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the packaged implementation for the matrix factorization model\n",
        "from recsys.recommendation import MFRecommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "eb8b18bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating MFRecommender with our actual dataset dimensions:\n",
            "Number of users: 268,300\n",
            "Number of movies: 41,239\n",
            "Embedding dimensions: 50\n",
            "\n",
            "Model created successfully!\n",
            "User embedding shape: torch.Size([268300, 50])\n",
            "Movie embedding shape: torch.Size([41239, 50])\n",
            "\n",
            "Example predictions:\n",
            "User 0, Movie 0 → Predicted rating: 0.038\n",
            "User 1, Movie 5 → Predicted rating: 0.017\n",
            "User 2, Movie 10 → Predicted rating: -0.005\n",
            "\n",
            "Note: These are random predictions since the model hasn't been trained yet!\n"
          ]
        }
      ],
      "source": [
        "# Example: Create and test our recommendation model\n",
        "print(\"Creating MFRecommender with our actual dataset dimensions:\")\n",
        "\n",
        "# Use the dimensions from our cleaned data\n",
        "num_users = len(training_user_ids)\n",
        "num_movies = len(training_movie_ids) \n",
        "embedding_dim = 50  # Start with 50 latent factors\n",
        "\n",
        "print(f\"Number of users: {num_users:,}\")\n",
        "print(f\"Number of movies: {num_movies:,}\")\n",
        "print(f\"Embedding dimensions: {embedding_dim}\")\n",
        "\n",
        "# Create the model\n",
        "model = MFRecommender(num_users, num_movies, embedding_dim)\n",
        "\n",
        "print(f\"\\nModel created successfully!\")\n",
        "print(f\"User embedding shape: {model.user_embedding.weight.shape}\")\n",
        "print(f\"Movie embedding shape: {model.item_embedding.weight.shape}\")\n",
        "\n",
        "# Test prediction for a few user-movie pairs\n",
        "test_users = torch.tensor([0, 1, 2])      # First 3 users\n",
        "test_movies = torch.tensor([0, 5, 10])    # Some movies\n",
        "\n",
        "predicted_ratings = model(test_users, test_movies)\n",
        "print(f\"\\nExample predictions:\")\n",
        "for i, (user, movie, rating) in enumerate(zip(test_users, test_movies, predicted_ratings)):\n",
        "    print(f\"User {user.item()}, Movie {movie.item()} → Predicted rating: {rating.item():.3f}\")\n",
        "\n",
        "print(f\"\\nNote: These are random predictions since the model hasn't been trained yet!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
